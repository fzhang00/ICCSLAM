\chapter{Description of CC-EKF-SLAM}\label{ch:algorithm}

The proposed algorithm: Camera Centric EKF SLAM (CC-EKF-SLAM), is
described in this chapter. The inverse depth parameterization is
described first, then followed by the EKF models and procedures. The
overall flow chart of the algorithm, and the run time of the current
implementation are also given. The preliminary test result on real
flight data was published in \cite{zhang_obstacle_2012}.

The algorithm was implemented in Python programming
language\cite{_python_????}. An open source machine vision library
OpenCV\cite{_opencv_????} was utilized to perform machine vision tasks
such as feature extraction and tracking. The Shi-Tomasi corner
detector \cite{shi_good_1994} was used to extract corner features from
the image. The visual tracking is accomplished by the pyramid
implementation of the Lucas-Kanade optical flow method
\cite{bouguet_pyramidal_1999}, referred to as pyramid LK in the rest
of the thesis.

\section{Camera Centric Inverse Depth Parameterization}

\noindent A 3D scene point $\boldsymbol{p_{i}^{C}}$ can be defined by 6 parameters
with $i$ representing the landmark number. The superscript $C$
indicates that the parameters are represented in the camera reference frame.
\begin{equation}
\boldsymbol{p_{i}^{C}}=\begin{bmatrix}
x_{i}^{C} & y_{i}^{C} & z_{i}^{C} & \rho _{i} & \varphi _{i}^{C} & 
\theta _{i}^{C} 
\end{bmatrix}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=10cm, keepaspectratio=true]{./Figures/idp.jpg}
\caption{Inverse depth parameterization}
\label{fig:algo1}
\end{figure}

Figure \ref{fig:algo1} shows the landmark parameters definition in
inverse depth parameterization. The first three parameters
$[x_{i}^{C}, y_{i}^{C}, z_{i}^{C}]$ represent the initialization point
where the landmark is first observed. $\rho_{i} = 1/d_i$ is the
inverse distance from the initialization position to the landmark. The
elevation-azimuth pair $[\varphi_{i}^{C}, \theta_{i}^{C}]$ encodes a
unit vector pointing from the initialization point to the landmark.
The vector is given by
\begin{equation}
\label{eq:m}
\vec{\boldsymbol{m}}(\varphi_{i}^{C}, \theta_{i}^{C})=\begin{bmatrix}
\cos\varphi_{i}^{C}\cos\theta _{i}^{C} \\
\cos\varphi_{i}^{C}\sin\theta _{i}^{C} \\
\sin\varphi_{i}^{C}
\end{bmatrix}
\end{equation}

\noindent Given the a vector $[h_x, h_y, h_z]$, one can also find the
elevation-azimuth angles $[\varphi, \theta]$ by

\begin{equation}
\label{eq:m_inv_varphi}
\varphi 
=arctan\left(\frac{h_{z}}{\sqrt{h_x^2+h_y^2}}\right)
\end{equation}

\begin{equation}
\label{eq:m_inv_theta}
\theta =arctan\left(\frac{h_{y}}{h_{x}}\right)
\end{equation}


\section{Modeling the System with Extended Kalman 
Filter}

\subsection{Full State Vector}

The EKF full state vector is defined as 

\begin{equation}
\boldsymbol{x}=\begin{bmatrix}
\boldsymbol{OX_{W}^{C}} & \boldsymbol{c^{C}} & \boldsymbol{r^{C}} & 
\boldsymbol{p_{1}^{C}} & \boldsymbol{p_{2}^{C}} & \ldots 
\end{bmatrix}^T
\end{equation}

\noindent where $\boldsymbol{OX_{W}^{C}}= \begin{bmatrix}O_{x}^{C} &
  O_{y}^{C} & O_{z}^{C} & W_{x}^{C} & W_{y}^{C} &
  W_{z}^{C} \end{bmatrix}^{T}$ contains translation parameters
$\boldsymbol{O_{x,y,z}^{C}}$ and orientation parameters
$\boldsymbol{W_{x,y,z}^{C}}$ that represent the world frame position and
orientation referenced to the camera frame.
$\boldsymbol{c^{C}}$ and $\boldsymbol{r^{C}}$ represent the camera
translation and rotation motion frame by frame. $\boldsymbol{p_{i}^{C}}$
contains the $i^{th}$ landmark parameters described in the previous section.

\subsection{Prediction}\label{sec:prediction}

For a prediction step at time $k$, the world frame parameter
$\boldsymbol{OX_W^C}$ and landmarks parameters $\boldsymbol{p_i^C}$
are kept unchanged from time $k-1$. The camera parameters are updated
using the new inertial measurements: velocity $\boldsymbol{v^{C}}$,
acceleration $\boldsymbol{a^{C}}$, and rate of change
$\boldsymbol{w^{C}}$ in roll, pitch, and heading. The full state
vector at time $k$ is
\begin{equation}
\boldsymbol{\hat{x}_{k}^-}
=\begin{bmatrix}
\boldsymbol{OX_{W,k-1}^{C}} & 
\boldsymbol{c_{measured}^{C}} &
\boldsymbol{r_{measured}^{C}} & 
\boldsymbol{p_{1,k-1}^{C}} & 
\boldsymbol{p_{2,k-1}^{C}} & 
\ldots & 
\boldsymbol{p_{n,k-1}^C}
\end{bmatrix}^T
\end{equation}

\noindent where 

$$\boldsymbol{c_{measured}^{C}}=\boldsymbol{v_{measured}^{C}}\Delta t+ 
\frac{1}{2}\boldsymbol{a_{measured}^{C}}\Delta t^{2}$$
$$\boldsymbol{r_{measured}^{C}}=\boldsymbol{r_{k-1}^{C}}+ \boldsymbol{w_{measured}^{C}}$$

\noindent and $\Delta t$ is the time elapsed from frame to frame. 

\subsection{Measurement Model}\label{sec:measurement_model}

Each observed landmark is related to the camera motion through the
measurement model (Figure \ref{fig:measurement_model}). This
relationship enables a correction on the camera motion and landmark
parameters based on the landmark locations observed in the image.

\begin{figure}[h]
\centering
\includegraphics[width=12cm, keepaspectratio=true]{./Figures/measurement_model.jpg}
\caption{Measurement model}
\label{fig:measurement_model}
\end{figure}
\FloatBarrier
For a landmark $\boldsymbol{p_{i}^{C}}$, the vector $\boldsymbol{i^{C}}$
pointing from the predicted camera location to the landmark
initialization position is

\begin{equation}
\boldsymbol{i_{k}^{C}}=\begin{bmatrix}
x_{i}^{C} \\
y_{i}^{C} \\
z_{i}^{C} \\
\end{bmatrix}_{k}-\begin{bmatrix}
c_{x}^{C} \\
c_{y}^{C} \\
c_{z}^{C} \\
\end{bmatrix}_{k}
\end{equation}

The normalized vector pointing from the predicted camera position to the 
landmark at time k is then 
\begin{equation}
  \boldsymbol{h_{k}^{C}}=\boldsymbol{Q}^{-1}\left(\boldsymbol{r_{k}^{C}}\right)
  \left(\rho_{k}\boldsymbol{i_{k}^{C}}+
    \boldsymbol{m}\left(\varphi_k^{C},\theta _{k}^{C}\right)\right)
\end{equation}

\noindent where $\boldsymbol{Q}^{-1}(\boldsymbol{r_{k}^{C}})$ is the inverse
rotation matrix from the camera frame at time $k-1$ to camera frame at
time $k$. From vector $\boldsymbol{h_{k}^{C}}$, the landmark location on
image plane can be found by

\begin{equation}
\boldsymbol{h_{k}^{U}}= \begin{bmatrix}
u_{k} \\
v_{k} \\
\end{bmatrix}=\begin{bmatrix}
\frac{f_{x}h_{y,k}^{C}}{h_{x,k}^{C}} \\
\frac{f_{y}h_{z,k}^{C}}{h_{x,k}^{C}} \\
\end{bmatrix}
\end{equation}

\noindent where $f_{x}$ and $f_{y}$ is the scaling factor of the projection 
obtained through camera calibration.

Since the measurement model is non-linear, the equation must be
linearized to calculate the Kalman gain $K$. The detailed formulation for
linearization is given in Appendix \ref{sec:jac_measurement}.

\subsection{Composition}

The corrected state vector has all its parameters defined in camera
frame at time $k-1$. To keep the reference frame up-to-date, a coordinate
transformation is needed to transform all parameters, and the error matrix
into camera frame at step k so that the next cycle of tracking can be
continued. This step is referred to as the composition. To
differentiate the camera frame, a subscript $k$ and $k-1$ are added to
the reference frame indicator $C$.

World reference frame parameters are transformed using the equation

\begin{equation}
\begin{bmatrix}
O_{x}^{C_{k}} \\
O_{y}^{C_k} \\
O_{z}^{C_k} \\
\end{bmatrix}_{k}=\boldsymbol{Q}^{-1}(\boldsymbol{r_{k}^{C_{k-1}}})\left(
\begin{bmatrix}
O_{x}^{C_{k-1}} \\
O_{y}^{C_{k-1}} \\
O_{z}^{C_{k-1}} \\
\end{bmatrix}_{k}- \begin{bmatrix}
c_{x}^{C_{k-1}} \\
c_{y}^{C_{k-1}} \\
c_{z}^{C_{k-1}} \\
\end{bmatrix}_{k}\right)
\end{equation}

\noindent and
\begin{equation}
\begin{bmatrix}
W_{x}^{C_{k}} \\
W_{y}^{C_{k}} \\
W_{z}^{C_{k}} \\
\end{bmatrix}_{k}= \begin{bmatrix}
W_{x}^{C_{k-1}} \\
W_{y}^{C_{k-1}} \\
W_{z}^{C_{k-1}} \\
\end{bmatrix}_{k}-\boldsymbol{r_k^{C_{k-1}}}
\end{equation}
 
Landmark parameters are related to the previous reference frame by

\begin{equation}
\begin{bmatrix}
x_{i}^{C_{k}} \\
y_{i}^{C_{k}} \\
z_{i}^{C_{k}} \\
\end{bmatrix}_{k}=\boldsymbol{Q}^{-1}(\boldsymbol{r_k^{C_{k-1}}})\left(
\begin{bmatrix}
x_{i}^{C_{k-1}} \\
y_{i}^{C_{k-1}} \\
z_{i}^{C_{k-1}} \\
\end{bmatrix}_{k}- \begin{bmatrix}
c_{x}^{C_{k-1}} \\
c_{y}^{C_{k-1}} \\
c_{z}^{C_{k-1}} \\
\end{bmatrix}_{k}\right)
\end{equation}

\noindent and
\begin{equation}
\begin{bmatrix}
\rho_{i} \\
\varphi_{i}^{C_{k}} \\
\theta_{i}^{C_{k}} \\
\end{bmatrix}_{k}=
\begin{bmatrix}
\rho _{i} \\
\boldsymbol{m}^{-1}\left(\boldsymbol{Q}^{-1}(\boldsymbol{r_k^{C_{k-1}}})\boldsymbol{m}(\varphi _{i, k}^{C_{k-1}}, \theta _{i, k}^{C_{k-1}})\right) \\
\end{bmatrix}
\end{equation}

\noindent where $\rho_i$ is a scalar that doesn't require transformation.
$\boldsymbol{m}^{-1}$ represents the operator that converts
a unit vector back to $[\varphi, \theta]$ angles as defined in
Equations (\ref{eq:m_inv_varphi}) and (\ref{eq:m_inv_theta}).

The covariance matrix is also affected by this transformation. The new
covariance matrix is related to the old one by

\begin{equation}
\boldsymbol{P_{k}^{C_{k}}}=\boldsymbol{J_{C_{k-1}\to C_{k}}}\boldsymbol{P_{k}^{C_{k-1}}}\boldsymbol{J_{C_{k-1}\to C_{k}}^{T}}
\end{equation}

\noindent where $\boldsymbol{J_{C_{k-1} \to C_k}}$ is the Jacobian matrix of the
composition equations. Detail derivation of the Jacobian matrix is
given in Appendix \ref{sec:jac_composition}  

\subsection{Filter Initialization} \label{sec:filter_initialization}
\subsubsection{State Vector}

The state vector is initialized at the first frame. The world frame
position and orientation, camera motions, and the landmark
initialization points are all initialized to zero, with variance
equal to the smallest machine number.

\begin{equation}
\label{eq:OX_init}
\boldsymbol{OX_{W}^{C}}=\begin{bmatrix}0&0&0&0&0&0\end{bmatrix}^T 
\end{equation}

\begin{equation}
\boldsymbol{c^{C}}=\begin{bmatrix}0&0&0\end{bmatrix}^T
\end{equation}

\begin{equation}
\boldsymbol{r^{C}}=\begin{bmatrix}0&0&0\end{bmatrix}^T
\end{equation}

\begin{equation}
\label{eq:pi_init}
\boldsymbol{p_{i}^{C}}=\begin{bmatrix}0&0&0&\rho _{i}&\varphi_{i}^C&\theta_{i}^C\end{bmatrix}^T
\end{equation}

The inverse distance $\rho$ of all landmarks are initialized to 0.01
($d=100 meters$). The landmark elevation-azimuth angles $[\varphi _{i}^{C},
\theta _{i}^{C}]$ are extracted from landmark coordinates in the image
plane. A vector pointing from camera optical center to a landmark
can be defined by
\begin{equation}
\label{eq:init_landmark_unit_vec}
\boldsymbol{h^{C}}=\begin{bmatrix}
h_{x}^{C}\\
h_{y}^{C}\\
h_{z}^{C}\\
\end{bmatrix}
 = \begin{bmatrix}
1 \\
(u-c_x)/f_{x} \\
(v-c_y)/f_{y} \\
\end{bmatrix}
\end{equation}

\noindent where $[u, v]$ are the landmark coordinates in the image, $
[f_{x}, f_{y}]$ are the scaling factors of the projection from the scene
to the image plane, $[c_x, c_y]$ are the coordinates at which the optical axis
intersects the image plane. The elevation-azimuth angles $[\varphi
_{i}^{C}, \theta _{i}^{C}]$ can be directly calculated from
$\boldsymbol{h^{C}}$ using Equations (\ref{eq:m_inv_varphi}) and (\ref{eq:m_inv_theta})

\subsubsection{State Covariance Matrix}

As the world reference frame is defined by the camera reference frame
at time 0, it allows the filter to be initialized with minimum
variance, which helps to reduce the lower bound of the filter error
according to the EKF SLAM properties. The initial covariance matrix for the
world reference frame position and orientation, and the camera motion is

\begin{equation}
\label{eq:Pinit}
\boldsymbol{P}=\boldsymbol{I_{12\times 12}}\cdot \epsilon 
\end{equation}

\noindent where $\epsilon $ is the least significant bit (LSB) of a
computer.

The covariance of landmarks are added one by one as there is 
correlation between them. For every new landmark added, the new 
covariance matrix becomes

\begin{equation}
\label{eq:Pnew}
\boldsymbol{P_{new}}=\boldsymbol{J}\begin{bmatrix}
\boldsymbol{P_{old}} & \boldsymbol{0} \\
\boldsymbol{0} & \boldsymbol{R} \\
\end{bmatrix}
\boldsymbol{J}^{T}
\end{equation}

\noindent where $\boldsymbol{P_{old}}$ is the covariance matrix of the existing
state vector. The initial $\boldsymbol{P_{old}}$ before the addition of the first landmark is Equation (\ref{eq:Pinit}). Matrix $\boldsymbol{R}$ is given by.

\begin{equation}
\label{eq:R}
\boldsymbol{R}=\begin{bmatrix}
\sigma _{x_{i}^{C}} & & & & & & \\
 & \sigma _{y_{i}^{C}} & & & 0 & & \\
 & & \sigma _{z_{i}^{C}} & & & & \\
 & & & \sigma _{\rho } & & & \\
 & 0 & & & \sigma _{image} & & \\
 & & & & & \sigma _{image} & \\
\end{bmatrix}
 = \begin{bmatrix}
\epsilon & & & & & & \\
 & \epsilon & & & 0 & & \\
 & & \epsilon & & & & \\
 & & & 0.01 & & & \\
 & 0 & & & 1 & & \\
 & & & & & 1 & \\
\end{bmatrix} 
\end{equation}

\noindent where $[\sigma_{x_{i}}^{C}, \sigma_{y_{i}}^{C}, \sigma
_{z_{i}}^{C}]$ is the uncertainty of the camera optical center
position, initialized to $\epsilon$. $\sigma_{image}$ is the variance
of the landmark coordinate on the image plane, set to 1 pixel. $\sigma
_{\rho }$ is the uncertainty of the inverse distance. Because the
filter mainly deals with distant landmark, $ \sigma _{\rho }$ is
initialized to 0.01 to cover distances from 50 meters to infinity.

$\boldsymbol{J}$ in Equation (\ref{eq:Pnew}) is the Jacobian matrix
for the landmark initialization equations given in Equations
(\ref{eq:OX_init})-(\ref{eq:pi_init}), (\ref{eq:m_inv_varphi}), and
(\ref{eq:m_inv_theta}). Detailed formulation of the Jacobian matrix is
given in Appendix \ref{ch:appendix2}.



\subsection{CC-EKF-SLAM Flow Chart and Run Time}
The flow chart of the entire algorithm is shown in Figure
\ref{fig:flowchart}. The algorithm initialize the EKF by detecting 40
corner features from the 1$^{st}$ image frame, and adding the
associated landmarks into the EKF state vector. Starting from the
2$^{nd}$ image frame, CC-EKF-SLAM operates on
prediction-correction-composition cycles. The prediction is done as
described in Section \ref{sec:prediction}. At the same time, a
prediction is also made on the landmark coordinates on the image plane
according to the predicted camera motion. In correction process,
CC-EKF-SLAM feed the predicted landmark coordinates to the pyramid LK
algorithm so that the visual tracking algorithm can start the search
for matching pattern at the predicted coordinates. The benefit is
shorter search time, and higher accuracy when image illumination level
changes. Then, correction to the state vector is made by taking the
landmark coordinates returned by the pyramid LK as measurements.

To shorten processing time for each iteration, landmarks that move
out of the camera's field of view (FOV) are removed from the filter
state vector and covariance matrix. The removal is done by deleting
the parameters from the state vector, and the related rows and columns
of the state covariance matrix. All tracked landmarks are recorded
into a database. The deleted landmarks have their parameters updated
based on the camera motion without any further Kalman corrections.
Because the deleted landmarks remain in database and are continuously
updated, they can be re-injected into the state vector if they
re-enter the FOV of the camera.

While some landmarks are deleted, new landmarks are added. The
landmark addition occurs after the composition step of each
iteration, and follows the same procedure as the filter
initialization described in Section \ref{sec:filter_initialization}.


\begin{figure}[h]
\centering
\includegraphics[width=14cm, keepaspectratio=true]{./Figures/flow_chart.jpg}
\caption{Algorithm flow chart}
\label{fig:flowchart}
\end{figure}
\FloatBarrier

The prototype of non-optimized CC-EKF-SLAM was run on a laptop with
Windows 7 operating system installed. A list of hardware and software
configuration for the laptop is listed in table \ref{tab:hardware},
and the runtime of key component of CC-EKF-SLAM is listed in table
\ref{tab:runtime}. 

\begin{table}[h]
\caption{Hardware and software configuration for CC-EKF-SLAM prototyping}
\label{tab:hardware}
\centering
\begin{tabular}{|c|c|}
\hline
\multicolumn{2}{|c|}{\textbf{Hardware Configuration}} \\ \hline
CPU & Intel(R) Core(TM) i5-3210M CPU @ 2.50GHz \\ \hline
RAM & 6.00GB \\ \hline \hline
\multicolumn{2}{|c|}{\textbf{Software Configuration}} \\ \hline
Operation System & Windows 7 Home Premium Service Pack 1 64-bit \\ \hline
Python Distribution & Python(x,y) 2.7.3.1 (32-bit) \\ \hline
% Python Modules Used & Numpy 1.6.2 \\
%                     & Scipy 0.11.0 \\
%                     & OpenCV 2.4.2 \\
%                     & Matplotlib 1.1.1 \\
%                     & h5py 2.1.0 \\
%                     & PyQt4 4.9.5-2 \\
%                     & pyproj 1.9.4 \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Typical runtime for CC-EKF-SLAM}
\label{tab:runtime}
\centering
\begin{tabular}{|l|c|}
\hline
Operation & Time Elapsed\\ \hline
Visual feature extraction & 24.99 ms \\ \hline
EKF initialization & 23.00 ms \\ \hline
EKF prediction & 9.99 ms \\ \hline
EKF correction & 19.99 ms \\ \hline
Composition & 28.99 ms \\ \hline
\end{tabular}
\end{table}
\FloatBarrier

\section{Additional Procedures for Data Analysis}
% word checked.
To compare the estimated SUAS poses and landmark positions to the ground
truth, all data must be referenced to the same reference frame. For
ease of viewing, the reference frame used for data analysis is the
world frame defined by the camera frame at time 0. 

The longitude and latitude of the SUAS position were first converted
to UTM using the WGS84 world geodetic system \cite{_world_????}. Many
software packages are readily available to do the conversion by taking GPS
coordinates and zone number as input. The software used in this work is
a Python interface to PROJ.4 library \cite{_pyproj_????} called pyproj
\cite{_pyproj_????}. Secondly, the ground truth of the SUAS positions
were brought to the world frame by reference frame transformation.

$$\text{True SUAS position}^W = 
Q_{step2}^{-1}Q_{step1}^{-1} \cdot\text{SUAS position}^{UTM}$$

\noindent where $Q_{step1}^{-1}$ and $Q_{step2}^{-1}$ is the inverse
transformation matrix for a reference frame evolving from UTM to world
frame. The formulation for coordinate transformation is given in
Appendix \ref{ch:appendix1}. The UTM to world frame transformation is
illustrated in Figure \ref{fig:utm_to_world}, and the transformation
parameters for step 1 and step 2 are also listed.

\begin{figure}[h]
  \centering
  \includegraphics[width=12cm,keepaspectratio=true]{./Figures/utm_to_world.jpg}
  \caption{Coordinate transformation from UTM to world frame}
  \label{fig:utm_to_world}
\end{figure}

$$T_{step1} = \begin{bmatrix}Easting_{init}, Northing_{init},
  Height_{init}\end{bmatrix}$$
$$R_{step1} = \begin{bmatrix}180^{\circ}, 0^{\circ},
  -90^{\circ}\end{bmatrix}$$
$$T_{step2} = \begin{bmatrix}0, 0, 0\end{bmatrix}$$
$$R_{step2} = \begin{bmatrix}Roll_{init}, Pitch_{init},
  Heading_{init}\end{bmatrix}$$

\noindent where $Easting_{init}$, $Northing_{init}$,
$Height_{init}$, $Roll_{init}$, $Pitch_{init}$, and
$Heading_{init}$ are the SUAS's position and orientation in UTM at time 0. 

Ground truth SUAS orientations were captured by the UAV navigation
unit GS-111m. Both ground truth and estimated orientation were
represented in aircraft principal axes. Hence, for the ground truth
SUAS orientation to be compared to the estimated data, the only step
is to subtract the orientation at time 0.

$$\text{SUAS orientation}_{\text{Ground truth}} =
\text{Orientation}^{Nav} - \text{Orientation}^{Nav}_0$$

The estimated SUAS poses can be obtained from the world reference
frame estimates in the filter state vector:

$$ \text{Estimated SUAS poses} = 
[-O_{XYZ}^{C}, -W_{XYZ}^{C}]$$.


To examine the accuracy of estimated landmark positions, DEM data were
converted into the world frame following the same procedure as the SUAS
position. Landmarks estimated from the CC-EKF-SLAM algorithm were
converted to the world frame using the estimated SUAS localization results
as reference frame evolving parameters. Let $[X_i^W,Y_i^W,
Z_i^W]^T_k$ be the $i^{th}$ landmarks coordinates in world frame at
step $k$, then

\begin{equation}
  \left[ \begin{array}{c}
    X_{i}^{W}  \\
    Y_{i}^{W}  \\
    Z_{i}^{W}  \\
  \end{array} \right]_k=Q^{-1}(\boldsymbol{O_{XYZ, k}^{c}}, \boldsymbol{W_{XYZ,k}^{c}})\left(\left[
    \begin{array}{c}
      x_{i}^{C} \\
      y_{i}^{C} \\
      z_{i}^{C} \\
    \end{array}
  \right]_k+\frac{1}{\rho _{i,k}}\boldsymbol{m}(\varphi_{i,k}^{C},\theta_{i,k}^{C})\right)
\end{equation}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
