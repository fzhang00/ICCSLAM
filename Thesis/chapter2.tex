\chapter{Literature Review}\label{ch:Review}

\section{Sensors for Obstacle Detection}\label{sec:sensor}

\subsection{Overview}\label{sec:SensorOverview}
% word checked
Many types of sensors can be used for obstacle detection such as ultrasonic
sensor, laser range finder, sonar, image sensor, or 3D flash LIDAR
\cite{de_angelis_low-cost_2007} \cite{alonge_novel_2009}
\cite{harb_neural_2008} \cite{saad_robust_2011}
\cite{williams_efficient_2001} \cite{chong_feature-based_1999}
\cite{hanna_obstacle_2008} \cite{lu_distance_2010}
\cite{civera_inverse_2008} \cite{jirawimut_visual_2003}
\cite{amzajerdian_lidar_2011} \cite{nieuwenhuisen_obstacle_2014}. Laser
range finders and ultrasonic sensors such as radar and sonar are
capable of high accuracy range measurement to centimeter level, but
only provide point measurement at a given position and orientation. To
acquire a full 3D range map of a scene, 2D mechanical scanning is
required, which limits the data acquisition rate of these devices.
LIDAR operates in the same manner as a laser range finder, but with the
scanning mechanism built in. Although LIDAR is becoming more and more
popular on land robots, it is usually expensive, heavy
\cite{subharsanan_low_2013}, and has a high power requirement when long
range measurement is required \cite{lemmens_airborne_2007}. These
characteristics make it unsuitable for a mid-size economical UAV. Sonar is
usually used in indoor or underwater applications, and has a wide beam
profile which makes it difficult to identify the origin of return
signals and results in a low resolution range map. 3D flash LIDAR is
capable of acquiring 3D range measurements simultaneously by illuminating
the entire field of view (FOV) of the camera with a single laser, and
capturing the reflected laser with a 2D imaging sensor
\cite{amzajerdian_lidar_2011}. However, its high cost has limited its
use in commercial applications. In recent years, many researchers have used
image sensors as passive range sensors due to their low weight, and low
cost. With advances in computer vision technology, image sensors
have been successfully used for range mapping and obstacle detection
in a number of platforms \cite{einhorn_cant_2010}
\cite{hashimoto_detection_1996} \cite{yamaguchi_moving_2006}
\cite{zhang_obstacle_2012} \cite{maier_self-supervised_2011}
\cite{kubota_global_2007} \cite{xu_method_2009}
\cite{hanna_obstacle_2008} \cite{zhang_real-time_2012}
\cite{van_der_mark_stereo_2007} \cite{broggi_stereo_2011}
\cite{plumet_toward_2014}.

\subsection{Monocular Vision and Binocular Vision}
% word checked
Image sensors are bearing-only sensors, which provide measurement on
the direction of the landmarks, but not the range. Other sensors
mentioned above, such as radar, LIDAR, are range and bearing sensors.
The principle of range measurement for image sensors is through
triangulating a common scene point in two or more images captured.
There are two types of configuration: monocular, and binocular.

For a binocular camera setup, two cameras are placed apart from each
other with their relative geometry (baseline separation and optical
axis orientation) fixed and known. The same scene point must be
captured by both cameras simultaneously. When the position of a
landmark can be accurately found in both images, its disparity is
defined by the difference in coordinates of the landmark in the left and
right images. Then, the coordinates of the landmark in world can be
calculated by using disparity, and the relative geometry of the
cameras. Because a binocular camera configuration acquires two images
simultaneously, landmark distance can be obtained at time $0$. The
challenge in binocular range sensing is the correspondence between the
two images, i.e., finding the same scene point in left and right
images. In addition, because of the limited separation between the
cameras, and vibration noise caused by vehicle motion, binocular range
sensing is usually applied in short to medium range measurement.

For a monocular camera setup, only one camera is used with an odometry
sensor providing camera displacement measurement between frames. Then
a similar principle is applied to triangulate the common landmark in two
consecutive frames to measure the distance. If the camera is facing the
direction of travel, data association is less difficult as image
differences from frame to frame are usually small. Secondly, the
baseline separation can be selected according to the targeted object
distance by processing every frame, every other frame, or every n
frames. This flexibility allows a monocular configuration to be
used for longer range measurement. On the other hand, since monocular
camera captures 1 frame at a time, the earliest range measurement is
not available until two image frames are captured.

\subsection{Camera Calibration}

% word checked
The camera model relates measurements on the image plane to measurements of
the 3D world. Camera calibration is the process of estimating the camera
model, which includes projection geometry of a camera and distortion
model of the lens. These models define the \textit{intrinsic
  parameters} of a camera.

\subsubsection{Basic Projection Geometry}
% word checked
\begin{figure}[h]
\centering
\includegraphics[width=12cm, keepaspectratio=true]{./Figures/camera_model.jpg}
\caption{Pinhole camera model}
\label{figch2:cammodel}
\end{figure}

A 3D point $\boldsymbol{P}=(X, Y, Z)$ and the corresponding point
$\boldsymbol{p}=(x, y)$ on the image plane (Figure
\ref{figch2:cammodel}), are related by equation \ref{eq:cam_projection}
\cite{bradski_learning_2008}

\begin{equation}
\label{eq:cam_projection}
\begin{matrix}
x = f_x\left(\frac{X}{Z}\right)+c_x, &
y=f_y\left(\frac{Y}{Z}\right)+c_y
\end{matrix}
\end{equation}

\noindent where $(c_x, c_y)$ is the pixel coordinate at which the optical
axis intersects the image plane; $f_x$ and $f_y$ (in pixels) are the product
of focal length $f$ (in millimeters) and the world-to-image scaling
factor $s_x$ and $s_y$ (in pixels/millimeter) for the X and Y axes. The
parameters $c_x$, $c_y$, $f_x$, $f_y$ define the camera intrinsic
matrix \cite{bradski_learning_2008} \cite{heikkila_four-step_1997}.

\begin{equation}
M = \begin{bmatrix}
f_x & 0 & c_x \\
0& f_y & c_y \\
0 & 0 & 1 \end{bmatrix}
\end{equation}

\subsubsection{Lens Distortion}
% word checked
There are two main contributors to lens distortion: radial distortion
and tangential distortion. Radial distortion arises as a result of the
shape of lens. It causes noticeable distortion to the pixels close to
the edge of the image, resulting in a ``fish-eye'' effect. The radial
distortion is zero at the center of the image, and increases with the
distance to the optical center. It is characterized by the first few
terms of a Taylor series expansion \cite{bradski_learning_2008}

\begin{equation}x_{corrected} = x(1+k_1r^2+k_2r^4+k_3r^6)\end{equation}
\begin{equation}y_{corrected} = y(1+k_1r^2+k_2r^4+k_3r^6)\end{equation}

\noindent where $r$ is the distance of the landmark from the optical
center on the image plane. Tangential distortion arises from
misalignment between the image sensor and the lens. It is minimally
characterized by two additional parameters $p_1$ and $p_2$
\cite{bradski_learning_2008}

\begin{equation}x_{corrected} = x+[2p_1y+p_2(r^2+2x^2)]\end{equation}
\begin{equation}y_{corrected} = y+[p_1(r^2+2y^2)+2p_2x]\end{equation}

\subsubsection{Camera Calibration Algorithm}
% word checked
Camera calibration is a process of finding the intrinsic parameters of
a camera. There are two popular camera calibration methods. Tsai's
method \cite{tsai_efficient_1986} requires at least 5 feature points
in 3D space with known coordinates, and their corresponding
coordinates on the image plane. With Tsai's method, only one snapshot
of the calibration target is needed. The disadvantage of this method
is that the 3D positions of the feature points are difficult to
obtain. Zhang \cite{zhang_flexible_2000} proposed a more flexible
method that requires at least two different views of a coplanar
target, which is usually a black and white checkerboard image.
However, to get a more reliable result, 10 views of the checkerboard
with different translations and rotations from the camera are desired.
The target's coordinates in the world frame need not be known in
advance. Instead, Zhang's algorithm only requires the size of the
square on the checkerboard. Zhang's method was implemented in OpenCV
\cite{bradski_learning_2008}, and was used to calibrate the camera for
this work.

% \section{Computer Vision Algorithms for Feature Detection and
%   Tracking}
% \textcolor{blue}{Need to add? The corner detection and optical flow
%   algorithm belongs to this category. No improvement was done on them.
% Just use them straight out of the opencv library. }
% \subsubsection{Corner Detector}
% %Direct copied: 
% Corners, by Harris��s definition, are places in the image where the
% autocorrelation matrix of the second derivatives has two large
% eigenvalues.Second derivatives are useful because they do not respond
% to uniform gradients.* Th is definition has the further advantage
% that, when we consider only the eigenvalues of the autocorrelation
% matrix, we are considering quantities that are invariant also to
% rotation, which is important because objects that we are tracking
% might rotate as well as move. Harris��s original definition involved
% taking the determinant of H(p), subtracting the trace of H(p) (with
% some weighting coefficient), and then comparing this difference to a
% predetermined threshold. It was later found by Shi and Tomasi [Shi94]
% that good corners resulted as long as the smaller of the two
% eigenvalues was greater than a minimum threshold. Shi and Tomasi��s
% method was not only sufficient but in many cases gave more
% satisfactory results than Harris��s method. (from opencv)

% \subsection{Limitation of Optical Sensor in Recursive Algorithm}
% \label{sec:OpticalSensorLimitation}

% \begin{itemize}
%   \item Error Accumulation over Iterations
%   \begin{itemize}
%     \item Feature quality Decreases over Iterations
%   \end{itemize}
% \end{itemize}

% \subsection{GPS and IMU}\label{sec:gps_and_imu}
% GPS and IMU are generally available on UAVs. These sensors provide a 
% measurement on the robot motion. Odometry can provide the scale 
% information which is missing in the bearing only measurement. 
% Furthermore, odometry provides some prior information on the robot 
% motion which can help to disambiguate the solution.

\section{SLAM as a Sensor Fusion Framework}
\label{sec:SLAM}
% word checked
An essential aspect of autonomy for a mobile robot is the capability
of obtaining a map and determining its location within it. This problem
has been referred to as simultaneous localization and mapping
(SLAM). There has been a considerable amount of research done on the SLAM
problem in the last two decades, and the theoretical SLAM can now be
considered solved. The standard structure of a SLAM problem is a
Bayesian form, and explains the evolution of the SLAM process. The most
common computational solution is through the use of an extended
Kalman filter (EKF).

\subsection{Recursive Probabilistic Estimation Using Extended Kalman Filter}
\label{sec:SLAM_using_EKF}
% word checked 

The Kalman filter \cite{kalman_new_1960}, published by R. E. Kalman in
1960, is a very powerful recursive data processing algorithm for
dynamic stochastic processes. The filter is extensively used in
control and navigation applications for its ability to estimate past,
present and even future states. It is an attractive candidate for a
data fusion framework as it can process all available measurements
including previous knowledge of the process, regardless of their
precision, to estimate the current value of the state variables. Given
a dynamic process that satisfies the assumptions that the Kalman
filter is based on, the filter is the optimal algorithm in minimizing
the mean squared error of the state variables. This section briefly
summarizes the assumptions and formulations of the Kalman filter which
is described in detail in \cite{sorenson_least-squares_1970}
\cite{analytic_sciences_corporation_applied_1974}
\cite{grewal_kalman_1993} \cite{lewis_optimal_1986}
\cite{brown_introduction_1993}. A more intuitive introduction can be
found in chapter 1 of \cite{maybeck_stochastic_1979}.

%reference in Welch and bishop
% intro to Kalman filter.

A Kalman filter has three assumptions. 1) \textit{The system model is
  linear}. The linearity is desired in that the system model is more
easily manipulated with engineering tool. When nonlinearity exists,
the typical approach is to linearize the system model at some nominal
points. 2) \textit{The noise embedded in system control and
  measurements is white}. This assumption implies that the noise value
is not correlated in time, and has equal power at all frequencies. 3)
\textit{The probability density function (PDF) of system and
  measurement noise is Gaussian}. A Gaussian distribution is fully
represented by the first and second order statistic (mean and
variance) of a process. Most other PDFs require many higher 
order of statistics to describe the shape fully. Hence, when the PDF
of a noise process is non-Gaussian, the Kalman filter that propagates
the first and second order statistics only includes partial information
of the PDF.

\subsubsection{Kalman Filter Models}
% word checked

The Kalman filter requires two models: a process model and a measurement
model. The process model defines a discrete-time controlled process
using a linear stochastic difference equation.
\begin{equation}
\text{Process Model: }
\boldsymbol{x_k} = \boldsymbol{Ax_{k-1}}+
\boldsymbol{B}\boldsymbol{\mu_{k-1}}+\boldsymbol{w_{k-1}}
\end{equation}
\noindent The $n\times n$ matrix $\boldsymbol{A}$ relates the state
variables $\boldsymbol{x_{k-1}}$ in the previous time step $k-1$ to the
state variable $\boldsymbol{x_{k}}$ in the current time step $k$. The
matrix $\boldsymbol{B}$ relates the optional control input $\boldsymbol{\mu}$
to the state variables $\boldsymbol{x}$. Given measurement vector
$\boldsymbol{z_k}$ of size $m \times 1$, the measurement model relates the
state variables to the measurements by matrix H of size $m \times n$.
\begin{equation}
\text{Measurement Model: }\boldsymbol{z_k} = \boldsymbol{Hx_k}+\boldsymbol{v_k}
\end{equation}
The random variables $\boldsymbol{w}$ and $\boldsymbol{v}$ represent the
uncertainty or noise of the process model, and the measurement.
$\boldsymbol{w}$ and $\boldsymbol{v}$ are assumed to be unrelated to each
other, and have Gaussian distribution with covariance $\boldsymbol{Q}$ and $\boldsymbol{R}$.

\subsubsection{Kalman Filter Algorithm}
% word checked
\begin{figure}[h]
\centering
\includegraphics[width=10cm, keepaspectratio=true]{./Figures/KalmanOperation.jpg}
\caption{Kalman filter operation flow diagram}
\label{figch2:1}
\end{figure}

The Kalman filter operates in prediction and correction cycles after
initialization (Figure \ref{figch2:1}). The state vector
$\boldsymbol{\hat{x}^-_k}$, $\boldsymbol{\hat{x}^+_k}$ contain a
priori and a posteriori estimates of the variables of interest at time
step k. $\boldsymbol{P^-_k}$ and $\boldsymbol{P^+_k}$ are the a priori
and a posteriori covariance matrices of the state vector. The
equations for prediction and correction cycles are listed in Table
\ref{tab:KF}. In prediction, an estimate of the state vector is made
based on the known process model. Since there are always unknown
factors not being fully described by the process model, the errors
almost always increase in the prediction. During correction, a number
of steps are carried out to correct the a priori estimate. First, the
predicted measurements $\boldsymbol{H\hat{x}^-_k}$, calculated through
the measurement model, are compared to the new measurements
$\boldsymbol{z_k}$. The difference
$\boldsymbol{z_k}-\boldsymbol{H\hat{x}^-_k}$ is called measurement
innovation, or residual. Next, the residuals are weighted by the
Kalman gain $\boldsymbol{K}$, and are added to
$\boldsymbol{\hat{x}^-_k}$ as correction. The Kalman gain is
formulated so that it minimizes the a posteriori error covariance
matrix $\boldsymbol{P^+_k}$.

\begin{table}
\caption{Kalman filter operation equations}
\label{tab:KF}
\centering
\begin{tabular}{|l|c r|}
  \hline
  \multirow{2}{*}{Prediction} 
  & $\boldsymbol{\hat{x}^-_k}=\boldsymbol{A\hat{x}^+_{k-1}}+\boldsymbol{B\mu_{k-1}}$ 
  & \stepcounter{equation}\thetag{\theequation}\\
  & $\boldsymbol{P^-_k} = \boldsymbol{AP^+_{k-1}A^T}+\boldsymbol{Q}$ 
  & \stepcounter{equation}\thetag{\theequation}\\
  \hline
  \multirow{3}{*}{Correction}
  & $\boldsymbol{K_k}=\boldsymbol{P^-_kH^T}(\boldsymbol{HP^-_kH^T}+\boldsymbol{R})^{-1}$  
  & \stepcounter{equation}\thetag{\theequation}\\
  & $\boldsymbol{\hat{x}^+_k}=\boldsymbol{\hat{x}^-_k}+\boldsymbol{K_k}(\boldsymbol{z_k}-\boldsymbol{H\hat{x}^-_k})$ 
  & \stepcounter{equation}\thetag{\theequation}\\
  & $\boldsymbol{P^+_k}=(\boldsymbol{I}-\boldsymbol{K_kH})\boldsymbol{P^-_k}$ 
  & \stepcounter{equation}\thetag{\theequation}\\
  \hline
\end{tabular}
\end{table}
\FloatBarrier

\subsubsection{Extended Kalman Filter}
% word checked
When a discrete-time controlled process, or its relationship with the
measurements is non-linear, a Kalman filter must be linearized about
the estimated trajectory. The filter is referred to as an extended
Kalman filter or EKF. A process with state vector $\boldsymbol{x}$ and
measurement $\boldsymbol{z}$ that is governed by a non-linear
stochastic difference equation has process and measurement model

\begin{equation}
\boldsymbol{x_k}=f(\boldsymbol{x_{k-1}}, \boldsymbol{\mu_{k-1}}, \boldsymbol{w_{k-1}}),
\end{equation}
\begin{equation}
\boldsymbol{z_k}=h(\boldsymbol{x_k}, \boldsymbol{v_k}),
\end{equation}

\noindent where the random variables $\boldsymbol{w_k}$ and $\boldsymbol{v_k}$ represent the
process and measurement noise with covariance $\boldsymbol{Q}$ and $\boldsymbol{R}$. The extended
Kalman filter operation equations are given in Table \ref{tab:EKF},

\begin{table}[H]
\caption{Extended Kalman filter operation equations}
\label{tab:EKF}
\centering
\begin{tabular}{|l|c r|}
\hline
\multirow{2}{*}{Prediction} 
& $\boldsymbol{\hat{x}}^-_k=f(\boldsymbol{\hat{x}^+_{k-1}},\boldsymbol{\mu_{k-1}},\boldsymbol{0})$ 
& \stepcounter{equation}\thetag{\theequation}\\
& $\boldsymbol{P^-_k} = \boldsymbol{A_kP^+_{k-1}A_k^T}+\boldsymbol{W_kQ_{k-1}W_k^T}$ & \stepcounter{equation}\thetag{\theequation}\\
\hline
\multirow{3}{*}{Correction}
& $\boldsymbol{K_k}=\boldsymbol{P^-_kH_k^T}(\boldsymbol{H_kP^-_kH_k^T}+\boldsymbol{V_kR_kV_k^T})^{-1}$  
& \stepcounter{equation}\thetag{\theequation}\\
& $\boldsymbol{\hat{x}^+_k}=\boldsymbol{\hat{x}^-_k}+\boldsymbol{K_k}(\boldsymbol{z_k}-h(\boldsymbol{\hat{x}^-_k},\boldsymbol{0}))$ 
& \stepcounter{equation}\thetag{\theequation}\\
& $\boldsymbol{P^+_k}=(\boldsymbol{I}-\boldsymbol{K_kH})\boldsymbol{P^-_k}$ 
& \stepcounter{equation}\thetag{\theequation}\\
\hline
\end{tabular}
\end{table}
\FloatBarrier

\noindent where (subscript $k$ is omitted)
\begin{itemize}
  \item $\boldsymbol{A}$ is the Jacobian matrix of partial derivatives of $f$ with
  respect to $\boldsymbol{x}$, 
\begin{equation}A_{[i,j]}= \frac{\partial f_i(\boldsymbol{\hat{x}_{k-1}^+},
    \boldsymbol{\mu_{k-1}}, 0)}{\partial x_j}\end{equation}
  \item $\boldsymbol{W}$ is the Jacobian matrix of partial derivatives of $f$ with
  respect to $\boldsymbol{w}$, 
\begin{equation}W_{[i,j]}= \frac{\partial f_i(\boldsymbol{\hat{x}_{k-1}^+},
    \boldsymbol{\mu_{k-1}}, 0)}{\partial w_j}\end{equation}
  \item $\boldsymbol{H}$ is the Jacobian matrix of partial derivatives of $h$ with
  respect to $\boldsymbol{x}$, 
\begin{equation}H_{[i,j]}= \frac{\partial h_i(\boldsymbol{\hat{x}_k^-},
    0)}{\partial x_j}\end{equation}
  \item $\boldsymbol{V}$ is the Jacobian matrix of partial derivatives of $h$ with
  respect to $\boldsymbol{v}$, 
\begin{equation}V_{[i,j]}= \frac{\partial
    h_i(\boldsymbol{\hat{x}_k^-},0)}{\partial v_j}\end{equation}
\end{itemize}

\noindent Note that when $\boldsymbol{w}$ and $\boldsymbol{v}$ directly describe the noise of
state vector and measurement, the Table \ref{tab:EKF} is the same as
Table \ref{tab:KF}.

\subsubsection{Tuning}
% word checked
The tuning of a Kalman filter can be achieved by adjusting the noise
covariance matrices $\boldsymbol{Q}$ and $\boldsymbol{R}$. The
measurement noise covariance $\boldsymbol{R}$ is usually easy to
estimate, i.e., by taking some offline measurements to compute the
covariance. On the other hand, process noise covariance
$\boldsymbol{Q}$ is more difficult. One common way is to inject enough
uncertainty into the process noise, and rely on reliable measurements.
 
\subsection{SLAM with Extended Kalman Filter}
% word checked
A simultaneous localization and mapping (SLAM) algorithm allows a
robot to build a map of its surrounding, and simultaneously
determining its location within the map. The mapping is achieved by
estimating the position of a number of landmarks, which are arbitrary
points in the environment that are detected and tracked by the
algorithm. When sufficient amount of landmarks can be established, and
evenly distributed among the scene, a high resolution map can be
obtained. 

The structure of the SLAM problem has evolved to a standard
Bayesian form. The EKF implementation of a SLAM problem is reviewed in
this section. A more complete tutorial to the SLAM problem is given in
\cite{durrant-whyte_simultaneous_2006}
\cite{bailey_simultaneous_2006}.

\subsubsection{General EKF Model for SLAM}
% word checked
Previous research showed that there is a high degree of correlation
between the estimates of the landmarks
\cite{smith_representation_1986} \cite{durrant-whyte_uncertain_1988}.
This correlation exists because of the common error in the estimated
vehicle poses \cite{leonard_simultaneous_1991}. The implication of
these works is that a consistent full solution to the SLAM problem
requires a joint state composed of the vehicle poses and all landmark
positions \cite{durrant-whyte_simultaneous_2006}. When the vehicle
poses and landmark positions are formulated as one single estimation
problem, the result is convergent. The correlation between landmarks
plays an important role in the quality of the solution. The more the
correlation grows, the better the solution becomes
\cite{durrant-whyte_localization_1996} \cite{csorba_new_1996}
\cite{csorba_simultaneous_1997} \cite{dissanayake_solution_2001}.

To formulate a SLAM problem, the following variables are defined at a given time step $k$
\begin{itemize}
  \item $\boldsymbol{x_k}$: a vector that describes the vehicle position and
orientation.
  \item $\boldsymbol{u_k}$: the control vector applied at time $k-1$ to drive the
vehicle to $\boldsymbol{x_k}$ at time step k
  \item $\boldsymbol{p_i}$: a vector that describes the location of the $i^{th}$
landmark. All landmarks locations are assumed to be time invariant.
  \item $\boldsymbol{z_{i,k}}$: observation of the location of the $i^{th}$
landmark taken from the vehicle's location at time step $k$
\end{itemize}

\noindent A complete state vector contains 

\begin{equation}\begin{bmatrix}\boldsymbol{\hat{x}_k} \\ \boldsymbol{\hat{p}_k} \end{bmatrix}\end{equation}

\noindent and the state covariance matrix contains

\begin{equation}\begin{bmatrix}
\boldsymbol{P_{xx}} & \boldsymbol{P_{xp}} \\
\boldsymbol{P_{px}} & \boldsymbol{P_{pp}} 
\end{bmatrix}\end{equation}

The prediction and correction cycles can then be carried out following
the standard EKF algorithm. The landmarks are generally not updated at
the prediction (i.e. only $\boldsymbol{x_k}$ and $\boldsymbol{P_{xx}}$ are
updated) unless they are moving.

\subsubsection{Properties of SLAM}
\label{sec:SLAM_properties}
% word checked
Dissanayake \cite{dissanayake_solution_2001} reached three important convergence properties of Kalman filter based SLAM, namely that: 
\textit{
\begin{enumerate}
  \item the determinant of any sub-matrix of the map covariance matrix decreases monotonically as observations are successively made;
  \item in the limit as the number of observations increases, the landmark estimates become fully correlated;
  \item in the limit the covariance associated with any single landmark location estimate reaches a lower bound determined only by the initial covariance in the vehicle location estimate at the time of the first sighting of the first landmark.
\end{enumerate}}

These results show that a complete knowledge of the cross-correlation
between landmark estimates is critical in maintaining the structure of
a SLAM problem. The errors in the estimates of landmarks become more and
more correlated as the vehicle ventures through the unknown
environment. The errors eventually become fully correlated which means
that given the location of one landmark, the location of any other
landmark can be determined with absolute certainty. As the map
converges, the errors in the estimates of landmarks reduce to a lower
bound determined by the errors of vehicle poses when the first
observation was made.

The results above only refer to the evolution of covariance matrices
computed by a linear model. In reality, most SLAM problems are nonlinear, and
the computed covariance does not match the true estimation error. This
leads to a SLAM consistency issue.

\subsubsection{Linearization Error and Consistency}
% word checked
Many researchers reported filter divergence due to linearization
errors \cite{martinelli_results_2005} \cite{julier_counter_2001}
\cite{bailey_consistency_2006} \cite{frese_discussion_2006}
\cite{castellanos_limits_2004}. As defined in
\cite{huang_analysis_2008}, a state estimator is consistent if the
estimation errors (i) are zero-mean, and (ii) have covariance matrix
smaller or equal to the one calculated by the filter. When covariance
reduces to a value smaller than the actual error, over-optimistic
estimation occurs and the filter can no longer update the estimates
effectively.

Huang investigated the properties and consistency of the nonlinear
two-dimensional EKF SLAM problem. He derived and proved a number of
theorems applicable to EKF SLAM. The conclusions are
\cite{huang_convergence_2007}: \textit{
\begin{itemize}
  \item Most of the convergence properties in 
  \cite{dissanayake_solution_2001} are still true for the nonlinear
  case provided that the Jacobians used in the EKF equations are
  evaluated at the true states.
  \item The main reasons for inconsistency in EKF SLAM are
  \begin{enumerate}
    \item the violation of some fundamental constraints governing the
    relationship between various Jacobians when they are evaluated at
    the estimated location, instead of the true location
    \item the use of relative location measurements from robot to
    landmarks to update the absolute robot and landmark location
    estimates in world coordinate.
  \end{enumerate}
  \item The robot orientation uncertainty plays an important role in
  both the EKF SLAM convergence and the possible inconsistency.
\end{itemize}
}

\subsection{SLAM for Building Large Scale Maps}
% word checked
The consistency issue of EKF SLAM limits its application in mapping a
large area. In response, a number of methods were proposed.

\subsubsection{Robot Centric Coordinate System}
Robot-centric mapping was proposed by Castellanos et al.
\cite{castellanos_limits_2004} and was adopted in
\cite{civera_1-point_2009} to create a large scale map on land. This
approach uses a reference frame attached to the robot as the base
frame. All geometric measurements and estimations were referred to the
robot frame. This method was shown to improve the consistency of the EKF
based solution to SLAM problem, but using local map joining to produce a large
map gives better result.

\subsubsection{Sub-map Methods}
The sub-map method is another way to tackle the large scale map problem.
The sub-maps can be either globally referenced or relatively
referenced. Global sub-maps estimate the location of the sub-map
referenced to a common base frame \cite{estrada_hierarchical_2005}
\cite{leonard_consistent_2003}. However, as the sub-map frames are
referred to a common base frame, a global sub-map does not improve the
consistency issue caused by the robot pose
uncertainty \cite{bailey_simultaneous_2006}. A relatively referenced
sub-map records its location referenced to the neighboring sub-maps
\cite{chong_feature-based_1999} \cite{williams_efficient_2001}. A global
map is then obtained through vector summation. The relatively
referenced sub-map is able to alleviate linearization problems in a large
scale map, but does not converge at a global level without an
independent sub-map structure. This problem can be solved by using
the Atlas framework or network coupled feature maps \cite{bosse_slam_2004}
\cite{bailey_mobile_2002}.

\section{Inverse Depth Parameterization for Distant Object}
The standard way of describing a landmark position is through the
Euclidean XYZ parameterization. In an outdoor range estimation
problem, the algorithm must deal with landmarks located at near
infinity, which give poor linearity when being represented in a
traditional Euclidean system. Furthermore, these landmarks cannot be
added to the filter until their locations have been determined with
good certainty. On the other hand, the benefit of including landmarks
at infinity is that they contribute in estimating the camera
rotational motion even though they offer little information on camera
translational motion. Inverse depth parameterization overcomes this
problem by representing range $d$ in its inverse form $\rho =1/d$. It
also allows for initializing long distance landmark into the EKF
framework before the landmark can be safely triangulated. The inverse
depth concept is widely used in computer vision for its relation with
the image disparity in stereo vision, or the optical flow vectors in
motion stereo \cite{okutomi_multiple-baseline_1991}
\cite{jepson_fast_1991} \cite{chowdhury_stochastic_2003}. Research
from Aidala et al. \cite{aidala_utilization_1983} shows that modified
polar coordinates leads to a stable extended Kalman filter in
bearing-only target motion analysis. Civera et al.
\cite{civera_inverse_2008} introduced an inverse depth
parameterization in polar coordinates. The inverse depth of a landmark
is referenced to the position at which the landmark was first
observed. This method results in measurement equations with high
degree of linearity.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:





