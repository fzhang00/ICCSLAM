\chapter{REVIEW}\label{ch:Review}

\section{SENSORS FOR OBSTACLE DETECTION}\label{sec:sensor}

\subsection{Overview}\label{sec:SensorOverview}
% word checked
Many sensors can be used for obstacle detection such as ultrasonic
sensor, laser range finder, sonar, image sensor, or 3D flash LIDAR
\cite{de_angelis_low-cost_2007} \cite{alonge_novel_2009}
\cite{harb_neural_2008} \cite{saad_robust_2011}
\cite{williams_efficient_2001} \cite{chong_feature-based_1999}
\cite{hanna_obstacle_2008} \cite{lu_distance_2010}
\cite{civera_inverse_2008} \cite{jirawimut_visual_2003}
\cite{amzajerdian_lidar_2011}. Laser range finder and ultrasonic
sensors such as radar and sonar are capable of high accuracy range
measurement up to centimeter level, but only provides point
measurement at a given position and orientation. To acquire full 3D
range map of a scene, 2D mechanical scanning are required, which
limits the data acquisition rate of these devices. LIDAR operates in
the same manner as laser range finder, except with the scanning
mechanism built in. Although LIDAR is becoming more and more popular
on land robot, it is usually expensive, heavy,
\cite{subharsanan_low_2013} and has high power requirement when long
range measurement is required \cite{lemmens_airborne_2007}. Sonar is
usually used in indoor or under water applications, and has wide beam
profile which makes it difficult to identify the origin of return
signal. This character results in a low resolution range map. 3D flash
LIDAR is capable of acquire 3D range measurement simultaneously by
illuminating the entire field of view (FOV) of the camera with a
single laser, and capturing the reflected laser with a 2D imaging
sensor \cite{amzajerdian_lidar_2011}. However, its high cost has
limited its use in commercial application. In recent years, many
researchers use image sensor as a passive range sensor for its low
weight, and low cost. With the advances in computer vision technology,
optical sensors have been successfully used for range mapping and
obstacle detection in a number of platforms \cite{einhorn_cant_2010}
\cite{hashimoto_detection_1996} \cite{yamaguchi_moving_2006}
\cite{zhang_obstacle_2012} \cite{maier_self-supervised_2011}
\cite{kubota_global_2007} \cite{xu_method_2009}
\cite{hanna_obstacle_2008} \cite{zhang_real-time_2012}
\cite{van_der_mark_stereo_2007} \cite{broggi_stereo_2011}.

\subsection{Monocular Vision and Binocular Vision}
% word checked
Image sensors are bearing-only sensors, which provide measurement on
the direction of the features, but not the range. Other sensors
mentioned above, such as radar, LIDAR, are range and bearing sensor.
The principle of range measurement for image sensors is through
triangulating a common scene point in two or more images captured.
There are two types of configuration: monocular, and binocular.

For binocular camera setup, two cameras are placed apart from each
other with their relative geometry (baseline separation and optical
axis orientation) known. The same scene must be captured by both
cameras simultaneously. When the position of a landmark can be
accurately found by both cameras, its disparity is defined by the
difference in image location of the landmark in the left and right
images. Then, the coordinate of the landmark from the cameras can be
calculated by using disparity, and the relative geometry of the
cameras. Because binocular setup acquires two images simultaneously,
the landmark distance can be obtained at time $0$. The challenge in
binocular range sensing is the correspondence problem between the two
images. In addition, because of the limited separation between the
camera, and the vibration noise caused by vehicle motion, binocular
range sensing are usually applied in short to medium range
measurement.

For monocular camera setup, only one camera is used with an odometry
sensors providing camera displacement measurement between frames. Then
similar principle applies to triangulate the common landmark in two
consecutive frames to measure the distance. If camera is facing the
direction of travel, data association is less difficult as image
difference from frame to frame is usually small. Secondly, the
baseline separation can be selected according to the targeted object
distance by processing every frame, every other frames, or every n
frames captured. This flexibility allows monocular configuration to be
used for longer range measurement. On the other hand, since monocular
camera captures 1 frame at a time, the earliest range measurement is
not available until two image frames were captured.

\subsection{Camera Calibration}

% word checked
Camera model relates measurement on image plane to measurement of the
3D world. Camera calibration is the process of estimating the camera
model, which includes projection geometry of a camera and distortion
model of the lens. These models define the \textit{intrinsic
  parameters} of a camera.

\subsubsection{Basic Projection Geometry}
% word checked
\begin{figure}[h]
\centering
\includegraphics[width=12cm, keepaspectratio=true]{./Figures/camera_model.jpg}
\caption{Pinhole camera model}
\label{figch2:cammodel}
\end{figure}

For a 3D point $P=(X, Y, Z)$ and the corresponding point
$p=(x, y)$ on the image plane (figure
\ref{figch2:cammodel}), they are related by equation
\cite{bradski_learning_2008}

\begin{equation}
\begin{matrix}
x = f_x\left(\frac{X}{Z}\right)+c_x, &
y=f_y\left(\frac{Y}{Z}\right)+c_y
\end{matrix}
\end{equation}

\noindent where $(c_x, c_y)$ is the
intersection of the optical axis and the image plane; $f_x$ and $f_y$
(in pixel) is the product of focal length $f$ (in millimeter) and the
scaling factor $s_x$ and $s_y$ (in pixel/millimeter) on X and Y axis.
The parameters $c_x$, $c_y$, $f_x$, $f_y$ define the camera intrinsic
matrix \cite{bradski_learning_2008} \cite{heikkila_four-step_1997}.

\begin{equation}
M = \begin{bmatrix}
f_x & 0 & c_x \\
0& f_y & c_y \\
0 & 0 & 1 \end{bmatrix}
\end{equation}

\subsubsection{Lens Distortion}
% word checked
There are two main contributors to lens distortion: radial distortion
and tangential distortion. Radial distortion arises as a result of the
shape of lens. It causes noticeable distortion to the pixels close to
the edge of the image, resulting in ``fish-eye'' effect. The radial
distortion is zero at the center of the image, and increases with the
distance to the optical center. It is characterized by the first few
terms of a Taylor series expansion \cite{bradski_learning_2008}

$$ x_{corrected} = x(1+k_1r^2+k_2r^4+k_3r^6)$$
$$ y_{corrected} = y(1+k_1r^2+k_2r^4+k_3r^6)$$

\noindent Tangential distortion arises from the misalignment between
the image sensor and the lens. It is minimally characterized by two
additional parameters $p_1$ and $p_2$ \cite{bradski_learning_2008}

$$x_{corrected} = x+[2p_1y+p_2(r^2+2x^2)]$$
$$y_{corrected} = y+[p_1(r^2+2y^2)+2p_2x]$$

\subsubsection{Camera Calibration Algorithm}
% word checked
Camera calibration is a process of finding the intrinsic parameters of
a camera. There are two popular camera calibration methods. Tsai
\cite{tsai_efficient_1986} requires at least 5 feature points in 3D
space with known coordinates, and their corresponding coordinates on
the image plane. With Tsai's method, only one snapshot of the
calibration target is needed. The disadvantage of this method is that
the 3D positions of the feature points are difficult to obtain. Zhang
\cite{zhang_flexible_2000} proposed a more flexible method that
requires at least two different views of a coplanar target, which is
usually a black and white checkerboard image. However, to get more
reliable result, 10 views of the checkerboard with different
translation and rotation from the camera is desired. The target's
coordinates in world frame need not to be known in advance. Instead,
algorithm only requires the size of the square on the checkerboard.
Zhang's method was implemented in OpenCV \cite{bradski_learning_2008},
and was used to calibrate the camera for this work.

\section{Computer Vision Algorithms for Feature Detection and
  Tracking}
\textcolor{blue}{Need to add? The corner detection and optical flow
  algorithm belongs to this category. No improvement was done on them.
Just use them straight out of the opencv library. }
% \subsubsection{Corner Detector}
% %Direct copied: 
% Corners, by Harris¡¯s definition, are places in the image where the
% autocorrelation matrix of the second derivatives has two large
% eigenvalues.Second derivatives are useful because they do not respond
% to uniform gradients.* Th is definition has the further advantage
% that, when we consider only the eigenvalues of the autocorrelation
% matrix, we are considering quantities that are invariant also to
% rotation, which is important because objects that we are tracking
% might rotate as well as move. Harris¡¯s original definition involved
% taking the determinant of H(p), subtracting the trace of H(p) (with
% some weighting coefficient), and then comparing this difference to a
% predetermined threshold. It was later found by Shi and Tomasi [Shi94]
% that good corners resulted as long as the smaller of the two
% eigenvalues was greater than a minimum threshold. Shi and Tomasi¡¯s
% method was not only sufficient but in many cases gave more
% satisfactory results than Harris¡¯s method. (from opencv)

% \subsection{Limitation of Optical Sensor in Recursive Algorithm}
% \label{sec:OpticalSensorLimitation}

% \begin{itemize}
%   \item Error Accumulation over Iterations
%   \begin{itemize}
%     \item Feature quality Decreases over Iterations
%   \end{itemize}
% \end{itemize}

% \subsection{GPS and IMU}\label{sec:gps_and_imu}
% GPS and IMU are generally available on UAVs. These sensors provide a 
% measurement on the robot motion. Odometry can provide the scale 
% information which is missing in the bearing only measurement. 
% Furthermore, odometry provides some prior information on the robot 
% motion which can help to disambiguate the solution.

\section{SLAM as A Sensor Fusion Framework}
\label{sec:SLAM}
% word checked
An essential aspect of autonomy for a mobile robot is the capability
of obtaining a map and determining its location within. This problem
has been referred to as the simultaneous localization and mapping
(SLAM). There were considerably amount of researches done on the SLAM
problem in the last two decades, and the theoretical SLAM can now be
considered solved. The standard structure of a SLAM problem is a
Bayesian form, and explains the evolution of the SLAM process. Most
common computational solution is through the use of an extended
Kalman filter (EKF).

\subsection{Recursive Probabilistic Estimation using Extended Kalman Filter}
\label{sec:SLAM_using_EKF}
% word checked 

The Kalman filter \cite{kalman_new_1960}, published by R. E. Kalman in
1960, is a very powerful recursive data processing algorithm for
dynamic stochastic processes. The filter is extensively used in
control and navigation application for its ability to estimate past,
present and even future state. It is an attractive candidate for data
fusion framework as it can process all available measurements
including previous knowledge of the process, regardless of their
precision, to estimate the current value of the variable of interest.
Given a dynamic process that satisfies the assumptions that Kalman
filter is based on, the filter is the optimal algorithm in minimizing
the mean of squared error of the state variable. This section briefly
summarized the assumptions and formulations of Kalman filter that's
described in detail in \cite{sorenson_least-squares_1970}
\cite{analytic_sciences_corporation_applied_1974}
\cite{grewal_kalman_1993} \cite{lewis_optimal_1986}
\cite{brown_introduction_1993}. A more intuitive introduction can be
found in chapter 1 of \cite{maybeck_stochastic_1979}.

%reference in Welch and bishop
% intro to Kalman filter.

The Kalman filter has three assumptions. 1) The system model is
linear. The linearity is desired in that the system model is more
easily manipulated with engineering tool. When nonlinearities do
exist, the typical approach is to linearize the system model at some
nominal points. 2) The noise embedded in system control and
measurements is white. This assumption implies that the noise value is
not correlated in time, and has equal power in all frequency. 3) The
probability density function (PDF) of system and measurement noise is
Gaussian. A Gaussian distribution is fully represented by the first
and second order statistic (mean and variance) of a process. Most
other densities require endless number of orders of statistic to
describe the shape fully. Hence, when the PDF of a noise process is
non-Gaussian, the Kalman filter that propagates the first and second
order statistic only includes partial information of the PDF.

\subsubsection{Kalman Filter Models}
% word checked

The Kalman filter requires two models: process model and measurement
model. The process model defines a discrete-time controlled process
using a linear stochastic difference equation.
\begin{equation}
\text{Process Model: }x_k = Ax_{k-1}+B\mu_{k-1}+w_{k-1}
\end{equation}
\noindent The $n\times n$ matrix $A$ relates the state variables
$x_{k-1}$ in previous time step $k-1$ to the state variable $x_{k}$ in
the current time step $k$. The matrix $B$ relates the optional control
input $\mu$ to the state variables $x$. Given measurement vector $z_k$
of size $1 \times m$, the measurement model relates the state
variables to the measurements by matrix H of size $n \times m$. 
\begin{equation}
\text{Measurement Model: }z_k = Hx_k+v_k
\end{equation}
The random variables $w$ and $v$ represent the uncertainty or noise of
the process model, and the measurement. $w$ and $v$ are assumed to be
unrelated to each other, and have Gaussian distribution with covariance
$Q$ and $R$.

\subsubsection{The Algorithm}
% word checked
The Kalman filter operates in prediction and correction cycles after
initialization (figure \ref{figch2:1}). The state vector
$\hat{x}^-_k$, $\hat{x}^+_k$ contain a priori and a posteriori
estimates of the variables of interest at time step k. $P^-_k$ and
$P^+_k$ are the a priori and a posteriori covariance matrices of the
state vector. The equations for prediction and correction cycles are
listed in table \ref{tab:KF}. In prediction, an estimate of the state
vector is made based on the known process model. Since there are
always unknown factors not being fully described by the process model,
the errors almost always increase in the prediction. During
correction, a number of steps are carried out to correct the a priori
estimate. First, the predicted measurements $H\hat{x}^-_k$, made
through the measurement model, are compared to the new measurements
$z_k$. The difference $z_k - H\hat{x}^-_k$ is called measurement
innovation, or residual. Next, the residuals are weighted by the
Kalman gain $K$, and are added to $\hat{x}^-_k$ as correction. The
Kalman gain is formulated so that it minimizes the a posteriori error
covariance matrix $P^+_k$.

\begin{figure}[h]
\centering
\includegraphics[width=10cm, keepaspectratio=true]{./Figures/KalmanOperation.jpg}
\caption{Kalman Operation Flow Diagram}
\label{figch2:1}
\end{figure}

\begin{table}
\caption{Kalman Filter Operation Equations}
\label{tab:KF}
\centering
\begin{tabular}{|l|c r|}
\hline
\multirow{2}{*}{Prediction} 
& $\hat{x}^-_k=A\hat{x}^+_{k-1}+B\mu_{k-1}$ & \stepcounter{equation}\thetag{\theequation}\\
& $P^-_k = AP^+_{k-1}A^T+Q$ & \stepcounter{equation}\thetag{\theequation}\\
\hline
\multirow{3}{*}{Correction}
& $K_k=P^-_kH^T(HP^-_kH^T+R)^{-1}$  & \stepcounter{equation}\thetag{\theequation}\\
& $\hat{x}^+_k = \hat{x}^-_k+K_k(z_k-H\hat{x}^-_k)$ & \stepcounter{equation}\thetag{\theequation}\\
& $P^+_k = (I-K_kH)P^-_k$ & \stepcounter{equation}\thetag{\theequation}\\
\hline
\end{tabular}
\end{table}
\FloatBarrier

\subsubsection{Extended Kalman Filter}
% word checked
When a discrete-time controlled process, or its relationship with the
measurements is non-linear, a Kalman filter must be linearized about
the estimated trajectory. The filter is referred to as an extended
Kalman filter or EKF. A process with state vector $x$ and measurement
$z$ that is governed by non-linear stochastic difference equation has
process and measurement model

\begin{equation}
x_k=f(x_{k-1}, u_{k-1}, w_{k-1}),
\end{equation}
\begin{equation}
z_k=h(x_k, v_k),
\end{equation}

\noindent where the random variables $w_k$ and $v_k$ represent the
process and measurement noise with variance $Q$ and $R$. The Extended
Kalman filter operation equations are given in table \ref{tab:EKF},

\begin{table}[H]
\caption{Extended Kalman Filter Operation Equations}
\label{tab:EKF}
\centering
\begin{tabular}{|l|c r|}
\hline
\multirow{2}{*}{Prediction} 
& $\hat{x}^-_k=f(\hat{x}^+_{k-1},\mu_{k-1},0)$ & \stepcounter{equation}\thetag{\theequation}\\
& $P^-_k = A_kP^+_{k-1}A_k^T+W_kQ_{k-1}W_k^T$ & \stepcounter{equation}\thetag{\theequation}\\
\hline
\multirow{3}{*}{Correction}
& $K_k=P^-_kH_k^T(H_kP^-_kH_k^T+V_kR_kV_k^T)^{-1}$  & \stepcounter{equation}\thetag{\theequation}\\
& $\hat{x}^+_k = \hat{x}^-_k+K_k(z_k-h(\hat{x}^-_k,0))$ & \stepcounter{equation}\thetag{\theequation}\\
& $P^+_k = (I-K_kH)P^-_k$ & \stepcounter{equation}\thetag{\theequation}\\
\hline
\end{tabular}
\end{table}
\FloatBarrier

\noindent where (subscript $k$ omitted)
\begin{itemize}
  \item $A$ is the Jacobian matrix of partial derivatives of $f$ with
  respect to $x$, $$A_{[i,j]}= \frac{\partial f_i(\hat{x}_{k-1}^+,
    \mu_{k-1}, 0)}{\partial x_j}$$
  \item $W$ is the Jacobian matrix of partial derivatives of $f$ with
  respect to $w$, $$W_{[i,j]}= \frac{\partial f_i(\hat{x}_{k-1}^+,
    \mu_{k-1}, 0)}{\partial w_j}$$
  \item $H$ is the Jacobian matrix of partial derivatives of $h$ with
  respect to $x$, $$H_{[i,j]}= \frac{\partial h_i(\hat{x}_k^-,
    0)}{\partial x_j}$$
  \item $V$ is the Jacobian matrix of partial derivatives of $h$ with
  respect to $v$, $$V_{[i,j]}= \frac{\partial
    h_i(\hat{x}_k^-,0)}{\partial v_j}$$
\end{itemize}

\noindent Note that when $w$ and $v$ directly describe the noise of
state vector and measurement, the table \ref{tab:EKF} is the same as
table \ref{tab:KF}.

\subsubsection{Tuning}
% word checked
The tuning of a Kalman filter can be achieved by adjusting noise covariance
matrix $Q$ and $R$. The measurement noise covariance $R$ is usually
easy to estimate, i.e., by taking some offline measurements to compute
the variance. On the other hand, process noise covariance $Q$ is more
difficult. One common way is to inject enough uncertainty into the
process noise, and rely on reliable measurements.
 
\subsection{SLAM with Extended Kalman Filter}
% word checked
The structure of SLAM problem has evolved to a standard
Bayesian form. The EKF implementation of a SLAM problem is reviewed in
this section. A more complete tutorial to the SLAM problem is given in
\cite{durrant-whyte_simultaneous_2006}
\cite{bailey_simultaneous_2006}.

\subsubsection{General EKF Model for SLAM}
% word checked
Previous researches show that there is a high degree of correlation
between the estimates of the landmarks
\cite{smith_representation_1986} \cite{durrant-whyte_uncertain_1988}.
This correlation exists because of the common error in the estimated
vehicle poses \cite{leonard_simultaneous_1991}. The implication of
these works is that a consistent full solution to the SLAM problem
requires a joint state composed of the vehicle pose and all landmarks
positions \cite{durrant-whyte_simultaneous_2006}. When the vehicle
pose and landmarks positions are formulated as one single estimation
problem, the result is convergent. The correlation between landmarks
plays an important role in the quality of the solution. The more the
correlation grows, the better the solution becomes.
\cite{durrant-whyte_localization_1996} \cite{csorba_new_1996}
\cite{csorba_simultaneous_1997} \cite{dissanayake_solution_2001}

At a given time step $k$, the following variables are defined
\begin{itemize}
  \item $x_k$: a vector that describes the vehicle position and
orientation.
  \item $u_k$: the control vector applied at time $k-1$ to drive the
vehicle to $x_k$ at time k
  \item $p_i$: a vector that describes the location of the $i^{th}$
landmark. All landmarks locations are assumed to be time invariant.
  \item $z_{i,k}$: observation of the location of the $i^{th}$
landmark taken from the vehicle's location at time $k$
\end{itemize}

\noindent Then a complete state vector contains 

$$\begin{bmatrix}\hat{x}_k \\ \hat{p}_k \end{bmatrix}$$ 

\noindent and the state covariance matrix contains

$$\begin{bmatrix}
P_{xx} & P_{xp} \\
P_{px} & P_{pp} 
\end{bmatrix} $$

The prediction and correction procedure can then be carried out
following the standard EKF formulation. The landmarks are generally
not updated at the prediction (i.e. only $x_k$ and $P_{xx}$ are
updated) unless they are moving.

\subsubsection{Properties of SLAM}
\label{sec:SLAM_properties}
% word checked
Dissanayake \cite{dissanayake_solution_2001} reached three important convergence properties of Kalman filter based SLAM, namely that: 
\textit{
\begin{enumerate}
  \item the determinant of any submatrix of the map covariance matrix decreases monotonically as observations are successively made;
  \item in the limit as the number of observations increases, the landmark estimates become fully correlated;
  \item in the limit the covariance associated with any single landmark location estimate reaches a lower bound determined only by the initial covariance in the vehicle location estimate at the time of the first sighting of the first landmark.
\end{enumerate}}

These results show that the complete knowledge of cross correlation
between landmark estimates is critical in maintaining the structure of
a SLAM problem. The error in estimates of landmarks becomes more and
more correlated as the vehicle venture through the unknown
environment. The error eventually becomes fully correlated which means
that given the location of one landmark, the location of any other
landmark can be determined with absolute certainty. As the map
converges, the error in the estimates of landmarks reduces to a lower
bound determined by the error when the first observation was made.

The results above only refer to the evolution of covariance matrices
computed by linear model. In reality, SLAM problem is nonlinear, and
the computed covariance does not match the true estimation error. This
leads to SLAM consistency issue.

\subsubsection{Linearization Error and Consistency}
% word checked
Many researchers reported and analyzed filter divergence due to
linearization error \cite{martinelli_results_2005},
\cite{julier_counter_2001}, \cite{bailey_consistency_2006},
\cite{frese_discussion_2006}, \cite{castellanos_limits_2004}. As
defined in \cite{huang_analysis_2008}, a state estimator is consistent
if the estimation errors (i) are zero-mean, and (ii) have covariance
matrix smaller or equal to the one calculated by the filter. When
covariance reduces to a value smaller than the actual error, over
optimistic estimation occurs and the filter can no longer effectively
update the estimates.

Huang investigated \cite{huang_convergence_2007} on properties and
consistency of nonlinear two-dimensional EKF based SLAM problem,
derived and proved a number of theorems applicable to EKF SLAM
convergence and consistency issue. Huang concluded that:

\begin{itemize}
  \item Most of the convergence properties in
  \cite{dissanayake_solution_2001} are still true for the nonlinear
  case provided that the Jacobians used in the EKF equations are
  evaluated at the true states.
  \item The main reasons for inconsistency in EKF SLAM are due to 
  \begin{enumerate}
    \item the violation of some fundamental constraints governing the
    relationship between various Jacobians when they are evaluated at
    the estimated location, instead of the true location
    \item the use of relative location measurements from robot to
    landmarks to update the absolute robot and landmark location
    estimates in world coordinate.
  \end{enumerate}
  \item The robot orientation uncertainty plays an important role in
  both the EKF SLAM convergence and the possible inconsistency.
\end{itemize}

\subsubsection{SLAM for Building Large Scale Maps}
% word checked
\paragraph{Robot Centric Coordinate System}In response to the
consistency problem of classic EKF SLAM solution, some researches
adopted the \emph{robotcentric mapping} \cite{castellanos_limits_2004}
\cite{civera_1-point_2009}. This approach uses a reference frame
attached to the robot as the base frame. All geometric measurements
and estimations are referred to the robot frame. This method was shown
to improve the consistency of EKF based solution to SLAM, but using
local map joining to produce a large map gives better result.

\paragraph{Submap Methods}The submap method is another mean to
tackle large scale map problem. The submaps can be either globally
referenced or relatively referenced. Global submaps estimate the
location of the submap referenced to a common base frame
\cite{estrada_hierarchical_2005} \cite{leonard_consistent_2003}.
However, as the submap frames are referred to a common base frame,
global submap does not improve the consistency issue caused by the
robot pose uncertainty.\cite{bailey_simultaneous_2006}. Relatively
referenced submap records its location referenced to the neighboring
submaps \cite{chong_feature-based_1999}
\cite{williams_efficient_2001}. Global map is then obtained through
vector summation. The relatively referenced submap is able to alleviate
linearization problem in large scale map, but does not converge on
global level without an independent submap structure. This problem can
be solved by using Atlas framework or network coupled feature maps
\cite{bosse_slam_2004} \cite{bailey_mobile_2002}

\section{Inverse Depth Parameterization for Distance Object}
The standard way of describing a landmark position is to use the
Euclidean XYZ parameterization. In outdoor range estimation
problem, the algorithm must deal with features located at near
infinity. Inverse depth parameterization overcomes this problem by
representing range $d$ in its inverse form $\rho =1/d$. In addition,
features at infinity contribute in estimating the camera rotational
motion even though they offer little information on camera
translational motion. Furthermore, inverse depth parameterization
allows for initializing features into the EKF framework before it is
safely triangulated. Inverse depth concept is widely used in computer
vision for its relation with the image disparity in stereo vision, and
the optical flow vectors in motion stereo
\cite{okutomi_multiple-baseline_1991}, \cite{jepson_fast_1991},
\cite{chowdhury_stochastic_2003}. Aidala et al.
\cite{aidala_utilization_1983} shows that modified polar coordinates
leads to a stable extended Kalman filter in bearing-only target motion
analysis. Civera et al. \cite{civera_inverse_2008} introduced an
inverse depth parameterization in polar coordinates. The inverse depth
of a landmark is referenced to the position at which the landmark was
first observed. This method results in measurement equations with high
degree of linearity.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:





